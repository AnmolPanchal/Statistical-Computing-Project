# -*- coding: utf-8 -*-
"""Project2-Final-AnmolPanchal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zlmp2_qmfdV3L7fXkRgDBNryT1vEGJxs

# Problem 1

## a > Updating rows
"""

import pandas as pd
import numpy as np
from scipy import linalg
import scipy.stats as st

df = pd.read_csv(r"C:\Users\anmol\Downloads\charlie.csv")

df = df[['x1', 'x2', 'x3', 'x4']]
X = np.array(df)
print(type(df))

x_test = X[0:4,:]
print(x_test)

n_rows, n_cols = x_test.shape
print(n_rows,n_cols)

updates = X[4:-1,:]

def Givens(a, b):
    if b == 0:
        c = 1
        s = 0
    elif np.abs(b) >= np.abs(a):
        t = -a/b
        s = 1/np.sqrt(1 + t**2)
        c = s*t
    else:
        t = -b/a
        c = 1/np.sqrt(1 + t**2)
        s = c*t
    return c, s

def update_rows(Q, R, u, data):
    next_row = u
    n_rows, n_cols = data.shape
    for j in range(n_cols):
        c, s  = Givens(R[j,j], next_row[j])
        R[j, j] = c*R[j, j] - s*next_row[j]
        #Update jth row of R and u
        t1 = R[j, j+1:n_cols]
        t2 = next_row[j + 1:n_cols]
        R[j, j+1:n_cols] = c*t1 - s*t2
        next_row[j+1:n_cols] = s*t1 + c*t2
    R_up = np.zeros((len(data)+1, len(data)+1))
    R_up[0:len(data), 0:len(data)] = R
#     R_up.shape
    
    m, n = Q.shape
    Q_up = np.zeros((m+1, m+1))
    Q_up[-1][-1] = 1
    Q_up[0:m, 0:m] = Q
    for j in range(n_cols):
        c, s = Givens(R[j, j], next_row[j])
        t1 = Q_up[0:m+1, j]
        t2 = Q_up[0:m+1, m]
        Q_up[0:m+1, j] = c*t1 - s*t2
        Q_up[0:m+1, m] = s*t1 + c*t2
    Q_up.shape
    
    data = np.vstack([data, np.zeros(n_cols)])
    data[-1] = u
    
#     A = np.delete(A, -1, 1)
#     print(A - data)
    print("Q update :", Q_up)
    print("R update: ", R_up)
#     print(np.dot(Q_up, R_up) - data)
    return Q_up, R_up

x_test

Q_up, R_up = np.linalg.qr(x_test)
nc=4

for i in range(4, len(X)):
    Q_up, R_up = update_rows(Q_up, R_up, X[i], x_test)
    A = np.dot(Q_up, R_up)
    r, c = np.shape(A)
    while c != nc:
        A = np.delete(A, -1, 1)
        c = c-1
    c = nc
    print("A: ",A)
    x_test = np.vstack([x_test, np.zeros(nc)])
    x_test[-1] = df.iloc[i,:]
    print("X_test = ", x_test, "\n")

A

Q_up,R_up

x_test



"""## b > updating sequentially column vise

covariance matrix of the set obtained with the first 2 columns and add sequentially the next 2 columns.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pylab
from pylab import rcParams
import sklearn

dataset = pd.read_csv(r"C:\Users\anmol\Downloads\charlie.csv")

x1 = np.array(dataset.x1)
x2 = np.array(dataset.x2)
x3 = np.array(dataset.x3)
x4 = np.array(dataset.x4)
dataset = np.array(dataset)

def gs_column_update(col_vector):
    update_array = []
    for v in col_vector:
        w = v - np.sum( np.dot(v,b)*b  for b in update_array )
        if (w > 1e-10).any():  
            update_array.append(w/np.linalg.norm(w))
#     return linalg.qr(np.array(update_array))
    return linalg.qr(np.cov(np.array(update_array)))
# You can remove np.cov from above line if you dont want the covariance matrix of the set obtained with sequential column updates.

gs_column_update([x1,x2])

gs_column_update([x1,x2,x3])

gs_column_update([x1,x2,x3,x4])

"""## c,d > Hn , aplha_n, R, plotting x_test data generated from classification"""





data = pd.read_csv(r"C:\Users\anmol\Downloads\charlie1.csv")
X = x_test
y = data['Data']
X = np.array(X)
y = np.array(y)
C,sigma = 10,0.001

def Kernel(x, y, sigma):
    return np.exp(-np.linalg.norm(x-y)**2 / ( (sigma ** 2)))

def Gram_Matrix(x):
    K = np.zeros((len(x),len(x)))
    for i in range(0, len(x)):
        for j in range(0, len(x)):
            K[i, j] = Kernel(x[i], x[j], sigma)
            
    return K

def H(x):
    mat = np.zeros((len(x), len(x)))
    mat[0:len(x), 0:len(x)] = Gram_Matrix(x) + np.eye(len(x))/2*C
    return mat

e = np.ones(len(X))
k = np.zeros((len(X)))

for j in range(0, len(X)):
    k[j] = Kernel(X[j], X[j], sigma)

H_mat = H(X)
al = alpha()

def alpha():
    p1 = np.dot(np.dot(np.linalg.inv(H_mat), e.T),k)
    p2 = np.dot(np.dot(np.linalg.inv(H_mat), e.T), e)
    p3 = (2-p1)/p2
    p3 = k + np.dot(p3, e)
    a = 0.5*np.dot(np.linalg.inv(H_mat),p3)
    return a

H(X)

Gram_Matrix(X)

alpha()

def R_square():
    p1 = 0
    p2 = 0
    total = 0
    for s in range(0, len(X)):
        k = Kernel(X[s], X[s], sigma)
        for j in range(0, len(X)):
            p1 = p1 + al[j]*Kernel(X[s], X[j], sigma)
            for l in range(0, len(X)):
                p2 = p2 + al[j]*al[l]*Kernel(X[j], X[l], sigma)
        total = total + (k - 2 * p1 + p2)
    final = total/len(X)
    return final

final = R_square()

final

def classification(x):
    t_out = []
    t_in = []
    p = 0
    p1 = 0
    for z in range(0, len(x)):
        k = Kernel(x[z], x[z], sigma)    
        for j in range(0, len(X)):
            p = p + al[j]*Kernel(x, X[j], sigma)
            for l in range(0, len(X)):
                p1 = p1 + al[j]*al[l]*Kernel(X[j], X[l], sigma)
        d = k - 2*p + p1
        if d <= final:
            t_in.append(x[z])
        else:
            t_out.append(x[z])

    return t_out, t_in

t_out, t_in = classification(X)

t_out,t_in

import matplotlib.pyplot as plt
import matplotlib.font_manager
from sklearn import svm

clf = svm.OneClassSVM(kernel = 'rbf', gamma = 'auto')
clf.fit(t_out,t_in)

n_error_outliers = t_out[t_out == 1].size
print("Number of errors = ",n_error_outliers,"/",y_out.size)
#classification rate
rate = n_error_outliers/y_out.size
print("Classification rate = ",100*(1-rate),"%")

import seaborn as sns
sns.pairplot(df)

type(t_out)

plot_data = pd.DataFrame(np.array(t_out).reshape(328,))

import seaborn as sns
sns.pairplot(plot_data)

sns.distplot(plot_data)

sns.clustermap(X)
# extra work not required.

sns.violinplot([X])
# extra work not required.



"""# Problem 2"""

# Load the digit data
digits = datasets.load_digits()
# View the features of the first observation
digits.data[0:1]
# View the target of the first observation
digits.target[0:1]
# Create dataset 1
data1_features = digits.data[:1000]
data1_target = digits.target[:1000]

# Create dataset 2
data2_features = digits.data[1000:]
data2_target = digits.target[1000:]

parameter_candidates = [
  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},
  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001,0.1,1,0.00001], 'kernel': ['rbf','linear']},
]

# Create a classifier object with the classifier and parameter candidates
clf = GridSearchCV(estimator=svm.SVC(), param_grid=parameter_candidates, n_jobs=-1)

# Train the classifier on data1's feature and target data
clf.fit(data1_features, data1_target)

# View the accuracy score
print('Best score for data1:', clf.best_score_)

# View the best parameters for the model found using grid search
print('Best C:',clf.best_estimator_.C) 
print('Best Kernel:',clf.best_estimator_.kernel)
print('Best Gamma:',clf.best_estimator_.gamma)

"""## We can verify my code above by implementing the code for best parameter selection which is provided on scikit learn"""

# http://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_digits.html

from __future__ import print_function

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.svm import SVC

print(__doc__)

# Loading the Digits dataset
digits = datasets.load_digits()

# To apply an classifier on this data, we need to flatten the image, to
# turn the data in a (samples, feature) matrix:
n_samples = len(digits.images)
X = digits.images.reshape((n_samples, -1))
y = digits.target

# Split the dataset in two equal parts
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.5, random_state=0)

# Set the parameters by cross-validation
tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],
                     'C': [1, 10, 100, 1000]},
                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]

scores = ['precision', 'recall']

for score in scores:
    print("# Tuning hyper-parameters for %s" % score)
    print()

    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,
                       scoring='%s_macro' % score)
    clf.fit(X_train, y_train)

    print("Best parameters set found on development set:")
    print()
    print(clf.best_params_)
    print()
    print("Grid scores on development set:")
    print()
    means = clf.cv_results_['mean_test_score']
    stds = clf.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, clf.cv_results_['params']):
        print("%0.3f (+/-%0.03f) for %r"
#               % (mean, std * 2, params))
    print()

    print("Detailed classification report:")
    print()
    print("The model is trained on the full development set.")
    print("The scores are computed on the full evaluation set.")
    print()
    y_true, y_pred = y_test, clf.predict(X_test)
    print(classification_report(y_true, y_pred))
    print()

# Note the problem is too easy: the hyperparameter plateau is too flat and the
# output model is the same for precision and recall with ties in quality.

